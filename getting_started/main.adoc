= Getting Started Guide
:ocp-ver: 4.13
:ocp-ver-full: 4.13.12
:prod-ver: 2.27
:podman-ver: 4.4.4
:ushift-ver: 4.13.12
:openshift-docs-url: https://access.redhat.com/documentation/en-us/openshift_container_platform/{ocp-ver}/html-single
:prod: Red Hat OpenShift Local
:rh-prod: Red Hat OpenShift Local
:attribute-missing: warn
:bin: crc
:centos: CentOS
:context: osl
:crc-download-url: https://console.redhat.com/openshift/create/local
:crc-gsg-url: https://access.redhat.com/documentation/en-us/red_hat_openshift_local/{prod-ver}/html-single/getting_started_guide/
:crc-gsg: Red Hat OpenShift Local Getting Started Guide
:crc-rn-ki-url: https://access.redhat.com/documentation/en-us/red_hat_openshift_local/{prod-ver}/html-single/release_notes_and_known_issues/
:crc-rn-ki: Red Hat OpenShift Local Release Notes and Known Issues
:debian: Debian
:experimental: true
:fed: Fedora
:mac: macOS
:msw: Microsoft Windows
:numbered: true
:oc-download-url: https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/
:oc-ver: v{ocp-ver-full}
:ocp: OpenShift Container Platform
:odo-docs-url-installing: https://access.redhat.com/documentation/en-us/openshift_container_platform/{ocp-ver}/html-single/cli_tools/index#installing-odo
:odo-docs-url-single-component: https://access.redhat.com/documentation/en-us/openshift_container_platform/{ocp-ver}/html-single/cli_tools/index#creating-a-single-component-application-with-odo
:odo-docs-url: https://access.redhat.com/documentation/en-us/openshift_container_platform/{ocp-ver}/html-single/cli_tools/index#understanding-odo
:openshift-docs-url-landing-page: https://access.redhat.com/documentation/en-us/openshift_container_platform/{ocp-ver}/html-single/getting_started
:openshift-installer-url: https://console.redhat.com/openshift/install
:openshift: OpenShift
:project-context: osl
:rh: Red Hat
:rhel-resolved-docs: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/using-different-dns-servers-for-different-domains_configuring-and-managing-networking
:rhel: Red Hat Enterprise Linux
:sectanchors: true
:sectlinks: true
:source-highlighter: prettify
:telemetry-notice-url: https://developers.redhat.com/article/tool-data-collection
:toc: left
:toclevels: 3
:ubuntu: Ubuntu
:ushift: MicroShift
:prod-ver-full: 2.27.0
:doctype: book
:page-component-name: getting_started
:page-component-version:
:page-version: {page-component-version}
:page-component-display-version: default
:page-component-title: Getting Started Guide

[preface]
== Making open source more inclusive

Red Hat is committed to replacing problematic language in our code, documentation, and web properties.
We are beginning with these four terms: master, slave, blacklist, and whitelist.
Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases.
For more details, see link:https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language[our CTO Chris Wright's message].

:docname: introducing
:page-module: getting_started
:page-relative-src-path: introducing.adoc
[#introducing]
== Introducing Red Hat OpenShift Local


[#about_red_hat_openshift_local]
=== About {prod}

{rh-prod} brings a minimal {ocp} 4 cluster and Podman container runtime to your local computer.
These runtimes provide minimal environments for development and testing purposes.
{prod} is mainly targeted at running on developers' desktops.
For other {ocp} use cases, such as headless or multi-developer setups, use the link:{openshift-installer-url}[full {openshift} installer].

See the link:{openshift-docs-url-landing-page}[{ocp} documentation] for a full introduction to {ocp}.

{prod} includes the [command]`{bin}` command-line interface (CLI) to interact with the {prod} instance using the desired container runtime.

[#differences_from_a_production_openshift_container_platform_installation]
=== Differences from a production {ocp} installation

The {openshift} preset for {rh-prod} provides a regular {ocp} installation with the following notable differences:

* **The {ocp} cluster is ephemeral and is not intended for production use.**
* **{prod} does not have a supported upgrade path to newer {ocp} versions.**
Upgrading the {ocp} version might cause issues that are difficult to reproduce.
* It uses a single node, which behaves as both a control plane and worker node.
* It disables the Cluster Monitoring Operator by default.
This disabled Operator causes the corresponding part of the web console to be non-functional.
* The {ocp} cluster runs in a virtual machine known as an __instance__.
This might cause other differences, particularly with external networking.

The {ocp} cluster provided by {prod} also includes the following non-customizable cluster settings.
These settings should not be modified:

* The ***.crc.testing** domain.
* The address range used for internal cluster communication.
** The cluster uses the **172** address range.
This can cause issues when, for example, a proxy is run in the same address space.


:docname: installing
:page-module: getting_started
:page-relative-src-path: installing.adoc
[#installing]
== Installing Red Hat OpenShift Local


[#minimum_system_requirements]
=== Minimum system requirements

[role="_abstract"]
{prod} has the following minimum hardware and operating system requirements.

[#hardware_requirements]
==== Hardware requirements

{prod} is supported on these architectures:

.Preset and architecture compatibility
|===
| Preset | AMD64 | Intel 64 | M1

| {ocp}
| yes
| yes
| yes

| {ushift}
| yes
| yes
| yes

| Podman container runtime
| yes
| yes
| yes

|===

{prod} does not support nested virtualization.

Depending on the desired container runtime, {prod} requires the following system resources:

[#for_openshift_container_platform]
===== For {ocp}

* 4 physical CPU cores
* 9 GB of free memory
* 35 GB of storage space

[#for_microshift]
===== For {ushift}

* 2 physical CPU cores
* 4 GB of free memory
* 35 GB of storage space

[NOTE]
====
The {ocp} and {ushift} presets require these minimum resources to run in the {prod} instance.
Some workloads might require more resources.
To assign more resources to the {prod} instance, see link:{crc-gsg-url}#configuring-the-instance_gsg[Configuring the instance].
====

[#for_the_podman_container_runtime]
===== For the Podman container runtime

* 2 physical CPU cores
* 2 GB of free memory
* 35 GB of storage space

[#operating_system_requirements]
==== Operating system requirements

{prod} requires the following minimum version of a supported operating system:

[#requirements_on_microsoft_windows]
===== Requirements on {msw}

* On {msw}, {prod} requires the Windows 10 Fall Creators Update (version 1709) or later.
{prod} does not work on earlier versions of {msw}.
{msw} 10 Home Edition is not supported.

[#requirements_on_macos]
===== Requirements on {mac}

* On {mac}, {prod} requires {mac} 11 Big Sur or later.
{prod} does not work on earlier versions of {mac}.

[#requirements_on_linux]
===== Requirements on Linux

* On Linux, {prod} is supported only on the latest two {rhel}/{centos} 8 and 9 minor releases and on the latest two stable {fed} releases.
* When using {rhel}, the machine running {prod} must be link:https://access.redhat.com/solutions/253273[registered with the Red Hat Customer Portal].
* {ubuntu} 18.04 LTS or later and {debian} 10 or later are not supported and might require manual set up of the host machine.
* See link:{crc-gsg-url}#required-software-packages_gsg[Required software packages] to install the required packages for your Linux distribution.

[#required_software_packages_for_linux]
=== Required software packages for Linux

{prod} requires the `libvirt` and `NetworkManager` packages to run on Linux.
Consult the following table to find the command used to install these packages for your Linux distribution:

.Package installation commands by distribution
[options="header"]
|====
|Linux Distribution|Installation command
|{fed}/{rhel}/{centos}|`sudo dnf install NetworkManager`
|{debian}/{ubuntu}|`sudo apt install qemu-kvm libvirt-daemon libvirt-daemon-system network-manager`
|====

[#installing_red_hat_openshift_local]
=== Installing {prod}

{prod} is available as a portable executable for {rhel}.
On {msw} and {mac}, {prod} is available using a guided installer.

.Prerequisites
* Your host machine must meet the minimum system requirements.
For more information, see link:{crc-gsg-url}#minimum-system-requirements_gsg[Minimum system requirements].

.Procedure
. Download the link:{crc-download-url}[latest release of {prod}] for your platform.

. On {msw}, extract the contents of the archive.

. On {mac} or {msw}, run the guided installer and follow the instructions.
+
[NOTE]
====
On {msw}, you must install {prod} to your local [filename]*_C:\_* drive.
You cannot run {prod} from a network drive.
====
+
On {rhel}, assuming the archive is in the [filename]*_~/Downloads_* directory, follow these steps:
+
.. Extract the contents of the archive:
+
[subs="attributes"]
----
$ cd ~/Downloads
$ tar xvf crc-linux-amd64.tar.xz
----
+
.. Create the [filename]*_~/bin_* directory if it does not exist and copy the [command]`{bin}` executable to it:
+
[subs="attributes"]
----
$ mkdir -p ~/bin
$ cp ~/Downloads/crc-linux-*-amd64/{bin} ~/bin
----
+
.. Add the [filename]*_~/bin_* directory to your `$PATH`:
+
[subs="attributes"]
----
$ export PATH=$PATH:$HOME/bin
$ echo 'export PATH=$PATH:$HOME/bin' >> ~/.bashrc
----

[#about_usage_data_collection]
=== About usage data collection

{prod} prompts you before use for optional, anonymous usage data collection to assist with development.
No personally identifiable information is collected.
Consent for usage data collection can be granted or revoked by you at any time.

[role="_additional-resources"]
.Additional resources
* For more information about collected data, see the {rh} link:{telemetry-notice-url}[Telemetry data collection notice].
* To grant or revoke consent for usage data collection, see link:{crc-gsg-url}#configuring-usage-data-collection_gsg[Configuring usage data collection].

[#configuring_usage_data_collection]
=== Configuring usage data collection

Consent for usage data collection can be granted or revoked by you at any time using the following configuration commands.

[NOTE]
====
Changes to telemetry consent do not modify a running instance.
The change will take effect next time you run the [command]`{bin} start` command.
====

.Procedure
* To manually enable telemetry, run the following command:
+
[subs="+quotes,attributes"]
----
$ {bin} config set consent-telemetry yes
----

* To manually disable telemetry, run the following command:
+
[subs="+quotes,attributes"]
----
$ {bin} config set consent-telemetry no
----

[role="_additional-resources"]
.Additional resources
* For more information about the collected data, see the {rh} link:{telemetry-notice-url}[Telemetry data collection notice].

[#upgrading_red_hat_openshift_local]
=== Upgrading {prod}

Newer versions of the {prod} executable require manual set up to prevent potential incompatibilities with earlier versions.

.Procedure
. link:{crc-download-url}[Download the latest release of {prod}].

. Delete the existing {prod} instance:
+
[subs="+quotes,attributes"]
----
$ {bin} delete
----
+
[WARNING]
====
The [command]`{bin} delete` command results in the loss of data stored in the {prod} instance.
Save any desired information stored in the instance before running this command.
====

. Replace the earlier [command]`{bin}` executable with the executable of the latest release.
Verify that the new [command]`{bin}` executable is in use by checking its version:
+
[subs="+quotes,attributes"]
----
$ {bin} version
----

. Set up the new {prod} release:
+
[subs="+quotes,attributes"]
----
$ {bin} setup
----

. Start the new {prod} instance:
+
[subs="+quotes,attributes"]
----
$ {bin} start
----


:docname: using
:page-module: getting_started
:page-relative-src-path: using.adoc
[#using]
== Using Red Hat OpenShift Local

[#about_presets]
=== About presets

[role="_abstract"]
{prod} presets represent a managed container runtime, and the lower bounds of system resources required by the instance to run it.
{prod} offers presets for:

`openshift`:: A minimal, preconfigured {ocp} {ocp-ver} cluster.
`microshift`:: {ushift}.
`podman`:: Podman container runtime.

On {msw} and {mac}, the {prod} guided installer prompts you for your desired preset.
On Linux, the {ocp} preset is selected by default.
You can change this selection using the [command]`{bin} config` command before running the [command]`{bin} setup` command.
You can change your selected preset from the system tray on {msw} and {mac} or from the command line on all supported operating systems.
Only one preset can be active at a time.

[role="_additional-resources"]
.Additional resources
* For more information about the minimum system requirements for each preset, see link:{crc-gsg-url}#minimum-system-requirements_gsg[Minimum system requirements].
* For more information on changing the selected preset, see link:{crc-gsg-url}#changing-the-selected-preset_gsg[Changing the selected preset].

[#setting_up_red_hat_openshift_local]
=== Setting up {prod}

[role="_abstract"]
The [command]`{bin} setup` command performs operations to set up the environment of your host machine for the {prod} instance.

The [command]`{bin} setup` command creates the [filename]*_~/.crc_* directory if it does not already exist.

[WARNING]
====
If you are setting up a new version, capture any changes made to the instance before setting up a new {prod} release.
====

.Prerequisites
* On Linux or {mac}, ensure that your user account has permission to use the [command]`sudo` command.
On {msw}, ensure that your user account can elevate to Administrator privileges.

[NOTE]
====
Do not run the [command]`{bin}` executable as the `root` user or an administrator.
Always run the [command]`{bin}` executable with your user account.
====

.Procedure
. (Optional) On Linux, the {ocp} preset is selected by default.
To select the Podman container runtime preset:
+
[subs="+quotes,attributes"]
----
$ {bin} config set preset podman
----

. Set up your host machine for {prod}:
+
[subs="+quotes,attributes"]
----
$ {bin} setup
----

[role="_additional-resources"]
.Additional resources
* For more information about the available container runtime presets, see link:{crc-gsg-url}#about-presets_gsg[About presets].

[#starting_the_instance]
=== Starting the instance

The [command]`{bin} start` command starts the {prod} instance and configured container runtime.

.Prerequisites
* To avoid networking-related issues, ensure that you are not connected to a VPN and that your network connection is reliable.
* You set up the host machine using the [command]`{bin} setup` command.
For more information, see link:{crc-gsg-url}#setting-up_gsg[Setting up {prod}].
* On {msw}, ensure that your user account can elevate to Administrator privileges.
* For the {openshift} preset, ensure that you have a valid {openshift} user pull secret.
Copy or download the pull secret from the Pull Secret section of the link:https://console.redhat.com/openshift/create/local[{prod} page on the {rh} Hybrid Cloud Console].
+
[NOTE]
====
Accessing the user pull secret requires a Red Hat account.
====

.Procedure
. Start the {prod} instance:
+
[subs="+quotes,attributes"]
----
$ {bin} start
----

. For the {openshift} preset, supply your user pull secret when prompted.
+
[NOTE]
====
The cluster takes a minimum of four minutes to start the necessary containers and Operators before serving a request.
====

.Additional resources
* To change the default resources allocated to the instance, see link:{crc-gsg-url}#configuring-the-instance_gsg[Configuring the instance].
* If you see errors during [command]`{bin} start`, see the link:{crc-gsg-url}#troubleshooting_gsg[Troubleshooting {prod}] section for potential solutions.

[#accessing_the_openshift_cluster]
=== Accessing the {openshift} cluster

Access the {ocp} cluster running in the {prod} instance by using the {ocp} web console or {openshift} CLI ([command]`oc`).

[#accessing_the_openshift_web_console]
==== Accessing the {openshift} web console

Access the {ocp} web console by using your web browser.

Access the cluster by using either the `kubeadmin` or `developer` user.
Use the `developer` user for creating projects or {openshift} applications and for application deployment.
Use the `kubeadmin` user only for administrative tasks such as creating new users or setting roles.

.Prerequisites
* {prod} is configured to use the {openshift} preset.
For more information, see link:{crc-gsg-url}#changing-the-selected-preset_gsg[Changing the selected preset].
* A running {prod} instance.
For more information, see link:{crc-gsg-url}#starting-the-instance_gsg[Starting the instance].

.Procedure
. To access the {ocp} web console with your default web browser, run the following command:
+
[subs="+quotes,attributes"]
----
$ {bin} console
----

. Log in as the `developer` user with the password printed in the output of the [command]`{bin} start` command.
You can also view the password for the `developer` and `kubeadmin` users by running the following command:
+
[subs="+quotes,attributes"]
----
$ {bin} console --credentials
----

See link:{crc-gsg-url}#troubleshooting_gsg[Troubleshooting {prod}] if you cannot access the {ocp} cluster managed by {prod}.

.Additional resources
* The link:https://docs.openshift.com/container-platform/latest/applications/projects/working-with-projects.html[{ocp} documentation] covers the creation of projects and applications.

[#accessing_the_openshift_cluster_with_the_openshift_cli]
==== Accessing the {openshift} cluster with the {openshift} CLI

Access the {ocp} cluster managed by {prod} by using the {openshift} CLI ([command]`oc`).

.Prerequisites
* {prod} is configured to use the {openshift} preset.
For more information, see link:{crc-gsg-url}#changing-the-selected-preset_gsg[Changing the selected preset].
* A running {prod} instance.
For more information, see link:{crc-gsg-url}#starting-the-instance_gsg[Starting the instance].

.Procedure
. Run the [command]`{bin} oc-env` command to print the command needed to add the cached [command]`oc` executable to your `$PATH`:
+
[subs="+quotes,attributes"]
----
$ {bin} oc-env
----

. Run the printed command.

. Log in as the `developer` user:
+
[subs="+quotes,attributes"]
----
$ oc login -u developer https://api.crc.testing:6443
----
+
[NOTE]
====
The [command]`{bin} start` command prints the password for the `developer` user.
You can also view it by running the [command]`{bin} console --credentials` command.
====

. You can now use [command]`oc` to interact with your {ocp} cluster.
For example, to verify that the {ocp} cluster Operators are available, log in as the `kubeadmin` user and run the following command:
+
[subs="+quotes,attributes",options="nowrap"]
----
$ oc config use-context crc-admin
$ oc whoami
kubeadmin
$ oc get co
----
+
[NOTE]
====
{prod} disables the Cluster Monitoring Operator by default.
====

See link:{crc-gsg-url}#troubleshooting_gsg[Troubleshooting {prod}] if you cannot access the {ocp} cluster managed by {prod}.

.Additional resources
* The link:https://docs.openshift.com/container-platform/latest/applications/projects/working-with-projects.html[{ocp} documentation] covers the creation of projects and applications.

[#accessing_the_internal_openshift_registry]
==== Accessing the internal {openshift} registry

The {ocp} cluster running in the {prod} instance includes an internal container image registry by default.
This internal container image registry can be used as a publication target for locally developed container images.
To access the internal {ocp} registry, follow these steps.

.Prerequisites
* {prod} is configured to use the {openshift} preset.
For more information, see link:{crc-gsg-url}#changing-the-selected-preset_gsg[Changing the selected preset].
* A running {prod} instance.
For more information, see link:{crc-gsg-url}#starting-the-instance_gsg[Starting the instance].
* A working {openshift} CLI ([command]`oc`) command.
For more information, see link:{crc-gsg-url}#accessing-the-openshift-cluster-with-oc_gsg[Accessing the {openshift} cluster with the {openshift} CLI].

.Procedure
. Check which user is logged in to the cluster:
+
[subs="+quotes,attributes"]
----
$ oc whoami
----
+
[NOTE]
====
For demonstration purposes, the current user is assumed to be `kubeadmin`.
====

. Log in to the registry as that user with its token:
+
[subs="+quotes,attributes"]
----
$ oc registry login --insecure=true
----

. Create a new project:
+
[subs="+quotes,attributes"]
----
$ oc new-project demo
----

. Mirror an example container image:
+
[subs="+quotes,attributes"]
----
$ oc image mirror registry.access.redhat.com/ubi8/ubi:latest=default-route-openshift-image-registry.apps-crc.testing/demo/ubi8:latest --insecure=true --filter-by-os=linux/amd64
----

. Get imagestreams and verify that the pushed image is listed:
+
[subs="+quotes,attributes"]
----
$ oc get is
----

. Enable image lookup in the imagestream:
+
[subs="+quotes,attributes"]
----
$ oc set image-lookup ubi8
----
+
This setting allows the imagestream to be the source of images without having to provide the full URL to the internal registry.

. Create a pod using the recently pushed image:
+
[subs="+quotes,attributes"]
----
$ oc run demo --image=ubi8 --command -- sleep 600s
----

[#deploying_a_sample_application_with_odo]
=== Deploying a sample application with `odo`

You can use [command]`odo` to create {openshift} projects and applications from the command line.
This procedure deploys a sample application to the {ocp} cluster running in the {prod} instance.

.Prerequisites
* You have installed [command]`odo`.
For more information, see link:{odo-docs-url-installing}[Installing `odo`] in the [command]`odo` documentation.
* {prod} is configured to use the {openshift} preset.
For more information, see link:{crc-gsg-url}#changing-the-selected-preset_gsg[Changing the selected preset].
* The {prod} instance is running.
For more information, see link:{crc-gsg-url}#starting-the-instance_gsg[Starting the instance].

.Procedure
. Log in to the running {ocp} cluster managed by {prod} as the `developer` user:
+
[subs="+quotes,attributes"]
----
$ odo login -u developer -p developer
----

. Create a project for your application:
+
[subs="+quotes,attributes"]
----
$ odo project create sample-app
----

. Create a directory for your components:
+
[subs="+quotes,attributes"]
----
$ mkdir sample-app
$ cd sample-app
----

. Clone an example Node.js application:
+
[subs="+quotes,attributes"]
----
$ git clone https://github.com/openshift/nodejs-ex
$ cd nodejs-ex
----

. Add a `nodejs` component to the application:
+
[subs="+quotes,attributes"]
----
$ odo create nodejs
----

. Create a URL and add an entry to the local configuration file:
+
[subs="+quotes,attributes"]
----
$ odo url create --port 8080
----

. Push the changes:
+
[subs="+quotes,attributes"]
----
$ odo push
----
+
Your component is now deployed to the cluster with an accessible URL.

. List the URLs and check the desired URL for the component:
+
[subs="+quotes,attributes"]
----
$ odo url list
----

. View the deployed application using the generated URL.

.Additional resources
* For more information about using [command]`odo`, see the link:{odo-docs-url}[`odo` documentation].

[#stopping_the_instance]
=== Stopping the instance

The [command]`{bin} stop` command stops the running {prod} instance and container runtime.
The stopping process takes a few minutes while the cluster shuts down.

.Procedure
* Stop the {prod} instance and container runtime:
+
[subs="+quotes,attributes"]
----
$ {bin} stop
----

[#deleting_the_instance]
=== Deleting the instance

The [command]`{bin} delete` command deletes an existing {prod} instance.

.Procedure
* Delete the {prod} instance:
+
[subs="+quotes,attributes"]
----
$ {bin} delete
----
+
[WARNING]
====
The [command]`{bin} delete` command results in the loss of data stored in the {prod} instance.
Save any desired information stored in the instance before running this command.
====

:docname: configuring
:page-module: getting_started
:page-relative-src-path: configuring.adoc
[#configuring]
== Configuring Red Hat OpenShift Local


[#about_red_hat_openshift_local_configuration]
=== About {prod} configuration

Use the [command]`{bin} config` command to configure both the [command]`{bin}` executable and the {prod} instance.
The [command]`{bin} config` command requires a subcommand to act on the configuration.
The available subcommands are `get`, `set,` `unset`, and `view`.
The `get`, `set`, and `unset` subcommands operate on named configurable properties.
Run the [command]`{bin} config --help` command to list the available properties.

You can also use the [command]`{bin} config` command to configure the behavior of the startup checks for the [command]`{bin} start` and [command]`{bin} setup` commands.
By default, startup checks report an error and stop execution when their conditions are not met.
Set the value of a property starting with `skip-check` to `true` to skip the check.

[#viewing_red_hat_openshift_local_configuration]
=== Viewing {prod} configuration

The {prod} executable provides commands to view configurable properties and the current {prod} configuration.

.Procedure
* To view the available configurable properties:
+
[subs="+quotes,attributes"]
----
$ {bin} config --help
----

* To view the values for a configurable property:
+
[subs="+quotes,attributes"]
----
$ {bin} config get _<property>_
----

* To view the complete current configuration:
+
[subs="+quotes,attributes"]
----
$ {bin} config view
----
+
[NOTE]
====
The [command]`{bin} config view` command does not return any information if the configuration consists of default values.
====

[#changing_the_selected_preset]
=== Changing the selected preset

[role="_abstract"]
You can change the container runtime used for the {prod} instance by selecting the desired preset.

On {msw} and {mac}, you can change the selected preset using the system tray or command line interface.
On Linux, use the command line interface.

[IMPORTANT]
====
You cannot change the preset of an existing {prod} instance.
Preset changes are only applied when a {prod} instance is created.
To enable preset changes, you must delete the existing instance and start a new one.
====

.Procedure
* Change the selected preset from the command line:
+
[subs="+quotes,attributes"]
----
$ {bin} config set preset __<name>__
----
+
Valid preset names are:
+
.Preset names
|===
| Name | Preset

| `openshift`
| {ocp}

| `microshift`
| {ushift}

| `podman`
| Podman container runtime

|===

[role="_additional-resources"]
.Additional resources
* For more information about the minimum system requirements for each preset, see link:{crc-gsg-url}#minimum-system-requirements_gsg[Minimum system requirements].

[#configuring_the_instance]
=== Configuring the instance

Use the `cpus` and `memory` properties to configure the default number of vCPUs and amount of memory available to the {prod} instance, respectively.

Alternatively, the number of vCPUs and amount of memory can be assigned using the `--cpus` and `--memory` flags to the `{bin} start` command, respectively.

[IMPORTANT]
====
You cannot change the configuration of a running {prod} instance.
To enable configuration changes, you must stop the running instance and start it again.
====

.Procedure
* To configure the number of vCPUs available to the instance:
+
[subs="+quotes,attributes"]
----
$ {bin} config set cpus __<number>__
----
+
The default value for the `cpus` property is `4`.
The number of vCPUs to assign must be greater than or equal to the default.

* To start the instance with the desired number of vCPUs:
+
[subs="+quotes,attributes"]
----
$ {bin} start --cpus __<number>__
----

* To configure the memory available to the instance:
+
[subs="+quotes,attributes"]
----
$ {bin} config set memory __<number-in-mib>__
----
+
[NOTE]
====
Values for available memory are set in mebibytes (MiB).
One gibibyte (GiB) of memory is equal to 1024 MiB.
====
+
The default value for the `memory` property is `9216`.
The amount of memory to assign must be greater than or equal to the default.

* To start the instance with the desired amount of memory:
+
[subs="+quotes,attributes"]
----
$ {bin} start --memory __<number-in-mib>__
----


:docname: networking
:page-module: getting_started
:page-relative-src-path: networking.adoc
[#networking]
== Networking


[#dns_configuration_details]
=== DNS configuration details

[#general_dns_setup]
==== General DNS setup

The {ocp} cluster managed by {prod} uses 2 DNS domain names, `crc.testing` and `apps-crc.testing`.
The `crc.testing` domain is for core {ocp} services.
The `apps-crc.testing` domain is for accessing {openshift} applications deployed on the cluster.

For example, the {ocp} API server is exposed as `api.crc.testing` while the {ocp} console is accessed as `console-openshift-console.apps-crc.testing`.
These DNS domains are served by a `dnsmasq` DNS container running inside the {prod} instance.

The [command]`{bin} setup` command detects and adjusts your system DNS configuration so that it can resolve these domains.
Additional checks are done to verify DNS is properly configured when running [command]`{bin} start`.

[#dns_on_linux]
==== DNS on Linux

On Linux, depending on your distribution, {prod} expects the following DNS configuration:

[#networkmanager_systemd_resolved]
===== NetworkManager + systemd-resolved

This configuration is used by default on Fedora 33 or newer, and on Ubuntu Desktop editions.

* {prod} expects NetworkManager to manage networking.
* {prod} configures `systemd-resolved` to forward requests for the `testing` domain to the `192.168.130.11` DNS server.
`192.168.130.11` is the IP of the {prod} instance.
* `systemd-resolved` configuration is done with a NetworkManager dispatcher script in [filename]*_/etc/NetworkManager/dispatcher.d/99-crc.sh_*:
+
----
#!/bin/sh

export LC_ALL=C

systemd-resolve --interface crc --set-dns 192.168.130.11 --set-domain ~testing

exit 0
----

[NOTE]
====
`systemd-resolved` is also available as an unsupported Technology Preview on {rhel} and {centos} 8.3.
After {rhel-resolved-docs}[configuring the host] to use `systemd-resolved`, stop any running clusters and rerun [command]`{bin} setup`.
====

[#networkmanager_dnsmasq]
===== NetworkManager + dnsmasq

This configuration is used by default on Fedora 32 or older, on {rhel}, and on {centos}.

* {prod} expects NetworkManager to manage networking.
* NetworkManager uses `dnsmasq` with the [filename]*_/etc/NetworkManager/conf.d/crc-nm-dnsmasq.conf_* configuration file.
* The configuration file for this `dnsmasq` instance is [filename]*_/etc/NetworkManager/dnsmasq.d/crc.conf_*:
+
----
server=/crc.testing/192.168.130.11
server=/apps-crc.testing/192.168.130.11
----

** The NetworkManager `dnsmasq` instance forwards requests for the `crc.testing` and `apps-crc.testing` domains to the `192.168.130.11` DNS server.

////
== {msw}

TODO
////

[#reserved_ip_subnets]
=== Reserved IP subnets

The {ocp} cluster managed by {prod} reserves IP subnets for internal use, which should not collide with your host network.
Ensure that the following IP subnets are available for use:

.Reserved IP subnets
* `10.217.0.0/22`
* `10.217.4.0/23`
* `192.168.126.0/24`

Additionally, the host hypervisor might reserve another IP subnet depending on the host operating system.
No additional subnet is reserved on {mac} and {msw}.
The additional reserved subnet for Linux is `192.168.130.0/24`.

[#starting_red_hat_openshift_local_behind_a_proxy]
=== Starting {prod} behind a proxy

You can start {prod} behind a defined proxy using environment variables or configurable properties.

[NOTE]
====
SOCKS proxies are not supported by {ocp}.
====

.Prerequisites
* If you are not using [command]`crc oc-env`, when interacting with the cluster, export the `.testing` domain as part of the `no_proxy` environment variable.
The embedded [command]`oc` executable does not require manual settings.
For more information about using the embedded [command]`oc` executable, see link:{crc-gsg-url}#accessing-the-openshift-cluster-with-oc_gsg[Accessing the {openshift} cluster with the {openshift} CLI].

.Procedure
. Define a proxy using the `http_proxy` and `https_proxy` environment variables or using the [command]`{bin} config set` command as follows:
+
[subs="+quotes,attributes"]
----
$ {bin} config set http-proxy http://proxy.example.com:__<port>__
$ {bin} config set https-proxy http://proxy.example.com:__<port>__
$ {bin} config set no-proxy __<comma-separated-no-proxy-entries>__
----

. If the proxy uses a custom CA certificate file, set it as follows:
+
[subs="+quotes,attributes"]
----
$ {bin} config set proxy-ca-file __<path-to-custom-ca-file>__
----

[NOTE]
====
Proxy-related values set in the configuration for {prod} have priority over values set with environment variables.
====

[#setting_up_red_hat_openshift_local_on_a_remote_server]
=== Setting up {prod} on a remote server

Configure a remote server to run an {ocp} cluster provided by {prod}.

This procedure assumes the use of a {rhel}, {fed}, or {centos} server.
Run every command in this procedure on the remote server.

[WARNING]
====
**Perform this procedure only on a local network.**
Exposing an insecure server on the internet has many security implications.
====

.Prerequisites
* {prod} is installed and set up on the remote server.
For more information, see link:{crc-gsg-url}#installing_gsg[Installing {prod}] and link:{crc-gsg-url}#setting-up_gsg[Setting up {prod}].
* {prod} is configured to use the {openshift} preset on the remote server.
For more information, see link:{crc-gsg-url}#changing-the-selected-preset_gsg[Changing the selected preset].
* Your user account has `sudo` permissions on the remote server.

.Procedure
. Start the cluster:
+
[subs="+quotes,attributes"]
----
$ {bin} start
----
+
Ensure that the cluster remains running during this procedure.

. Install the [package]`haproxy` package and other utilities:
+
----
$ sudo dnf install haproxy /usr/sbin/semanage
----

. Modify the firewall to allow communication with the cluster:
+
----
$ sudo systemctl enable --now firewalld
$ sudo firewall-cmd --add-service=http --permanent
$ sudo firewall-cmd --add-service=https --permanent
$ sudo firewall-cmd --add-service=kube-apiserver --permanent
$ sudo firewall-cmd --reload
----

. For SELinux, allow HAProxy to listen on TCP port 6443 to serve `kube-apiserver` on this port:
+
----
$ sudo semanage port -a -t http_port_t -p tcp 6443
----

. Create a backup of the default [application]`haproxy` configuration:
+
----
$ sudo cp /etc/haproxy/haproxy.cfg{,.bak}
----

. Configure [application]`haproxy` for use with the cluster:
+
[subs="+quotes,attributes"]
----
$ export CRC_IP=$({bin} ip)
$ sudo tee /etc/haproxy/haproxy.cfg &>/dev/null <<EOF
global
    log /dev/log local0

defaults
    balance roundrobin
    log global
    maxconn 100
    mode tcp
    timeout connect 5s
    timeout client 500s
    timeout server 500s

listen apps
    bind 0.0.0.0:80
    server crcvm $CRC_IP:80 check

listen apps_ssl
    bind 0.0.0.0:443
    server crcvm $CRC_IP:443 check

listen api
    bind 0.0.0.0:6443
    server crcvm $CRC_IP:6443 check
EOF
----

. Start the [application]`haproxy` service:
+
----
$ sudo systemctl start haproxy
----

[#connecting_to_a_remote_red_hat_openshift_local_instance]
=== Connecting to a remote {prod} instance

Use [application]`dnsmasq` to connect a client machine to a remote server running an {ocp} cluster managed by {prod}.

This procedure assumes the use of a {rhel}, {fed}, or {centos} client.
Run every command in this procedure on the client.

[IMPORTANT]
====
**Connect to a server that is only exposed on your local network.**
====

.Prerequisites
* A remote server is set up for the client to connect to.
For more information, see link:{crc-gsg-url}#setting-up-remote-server_gsg[Setting up {prod} on a remote server].
* You know the external IP address of the server.
* You have the link:{oc-download-url}[latest {openshift} CLI ([command]`oc`)] in your `$PATH` on the client.

.Procedure
. Install the [package]`dnsmasq` package:
+
----
$ sudo dnf install dnsmasq
----

. Enable the use of [application]`dnsmasq` for DNS resolution in NetworkManager:
+
----
$ sudo tee /etc/NetworkManager/conf.d/use-dnsmasq.conf &>/dev/null <<EOF
[main]
dns=dnsmasq
EOF
----

. Add DNS entries for {prod} to the [application]`dnsmasq` configuration:
+
[subs="+quotes"]
----
$ sudo tee /etc/NetworkManager/dnsmasq.d/external-crc.conf &>/dev/null <<EOF
address=/apps-crc.testing/__SERVER_IP_ADDRESS__
address=/api.crc.testing/__SERVER_IP_ADDRESS__
EOF
----
+
[NOTE]
====
Comment out any existing entries in [filename]*_/etc/NetworkManager/dnsmasq.d/crc.conf_*.
These entries are created by running a local instance of {prod} and will conflict with the entries for the remote cluster.
====

. Reload the NetworkManager service:
+
----
$ sudo systemctl reload NetworkManager
----

. Log in to the remote cluster as the `developer` user with [command]`oc`:
+
----
$ oc login -u developer -p developer https://api.crc.testing:6443
----
+
The remote {ocp} web console is available at \https://console-openshift-console.apps-crc.testing.


:docname: administrative-tasks
:page-module: getting_started
:page-relative-src-path: administrative-tasks.adoc
[#administrative-tasks]
== Administrative tasks


[#starting_monitoring]
=== Starting monitoring

{prod} disables cluster monitoring by default to ensure that {prod} can run on a typical notebook.
Monitoring is responsible for listing your cluster in the link:https://console.redhat.com/openshift[Red Hat Hybrid Cloud Console].
Follow this procedure to enable monitoring for your cluster.

.Prerequisites
* You must assign additional memory to the {prod} instance.
At least 14 GiB of memory, a value of `14336`, is recommended for core functionality.
Increased workloads will require more memory.
For more information, see link:{crc-gsg-url}#configuring-the-instance_gsg[Configuring the instance].

.Procedure
. Set the `enable-cluster-monitoring` configurable property to `true`:
+
[subs="+quotes,attributes"]
----
$ {bin} config set enable-cluster-monitoring true
----

. Start the instance:
+
[subs="+quotes,attributes"]
----
$ {bin} start
----
+
[WARNING]
====
Cluster monitoring cannot be disabled.
To remove monitoring, set the `enable-cluster-monitoring` configurable property to `false` and delete the existing {prod} instance.
====

[#enabling_override_operators]
=== Enabling override Operators

To verify {prod} can run on a typical notebook, some resource-heavy services get disabled by default.
These services can be enabled by manually removing the desired Operator from the Operator override list.

.Prerequisites
* A running {prod} virtual machine and a working [command]`oc` command.
For more information, see link:{crc-gsg-url}#accessing-the-openshift-cluster-with-oc_gsg[Accessing the OpenShift cluster with `oc`].
* You must log in through [command]`oc` as the `kubeadmin` user.

.Procedure
. List unmanaged Operators and note the numeric index for the desired Operator:

** On Linux or {mac}:
+
[subs="+quotes"]
----
$ oc get clusterversion version -ojsonpath='{range .spec.overrides[*]}{.name}{"\n"}{end}' | nl -v 0
----

** On {msw} using PowerShell:
+
[subs="+quotes"]
----
PS> oc get clusterversion version -ojsonpath='{range .spec.overrides[*]}{.name}{"\n"}{end}' | % {$nl++;"`t$($nl-1) `t $_"};$nl=0
----

. Start the desired Operator using the identified numeric index:
+
[subs="+quotes"]
----
$ oc patch clusterversion/version --type='json' -p '[{"op":"remove", "path":"/spec/overrides/_<unmanaged-operator-index>_"}]'
clusterversion.config.openshift.io/version patched
----


:docname: troubleshooting
:page-module: getting_started
:page-relative-src-path: troubleshooting.adoc
[#troubleshooting]
== Troubleshooting Red Hat OpenShift Local


[NOTE]
====
The goal of {rh-prod} is to deliver an {ocp} environment for development and testing purposes.
Issues occurring during installation or usage of specific {openshift} applications are outside of the scope of {prod}.
Report such issues to the relevant project.
====

[#getting_shell_access_to_the_openshift_cluster]
=== Getting shell access to the {openshift} cluster

To access the cluster for troubleshooting or debugging purposes, follow this procedure.

[NOTE]
====
Direct access to the {ocp} cluster is not needed for regular use and is strongly discouraged.
====

.Prerequisites
* Enable {openshift} CLI ([command]`oc`) access to the cluster and log in as the `kubeadmin` user.
For detailed steps, see link:{crc-gsg-url}#accessing-the-openshift-cluster-with-oc_gsg[Accessing the {openshift} cluster with the {openshift} CLI].

.Procedure
. Run the [command]`oc get nodes` command to identify the desired node.
The output will be similar to this:
+
[subs="+quotes,attributes",options="nowrap"]
----
$ oc get nodes
NAME                 STATUS   ROLES           AGE    VERSION
crc-shdl4-master-0   Ready    master,worker   7d7h   v1.14.6+7e13ab9a7
----

. Run [command]`oc debug nodes/_<node>_` where `_<node>_` is the name of the node printed in the previous step.

[#troubleshooting_expired_certificates]
=== Troubleshooting expired certificates

The system bundle of {ocp} in each released [command]`{bin}` executable expires 1 year after the release.
This expiration is due to certificates embedded in the {ocp} cluster.
The [command]`{bin} start` command triggers an automatic certificate renewal process when needed.
Certificate renewal can add up to five minutes to the start time of the cluster.

To avoid this additional startup time, or in case of failures in the certificate renewal process, use the following procedure:

.Procedure
To resolve expired certificate errors that cannot be automatically renewed:

. link:{crc-download-url}[Download the latest {prod} release] and place the [command]`{bin}` executable in your `$PATH`.

. Remove the cluster with certificate errors using the [command]`{bin} delete` command:
+
[subs="+quotes,attributes"]
----
$ {bin} delete
----
+
[WARNING]
====
The [command]`{bin} delete` command results in the loss of data stored in the {prod} instance.
Save any desired information stored in the instance before running this command.
====

. Set up the new release:
+
[subs="+quotes,attributes"]
----
$ {bin} setup
----

. Start the new instance:
+
[subs="+quotes,attributes"]
----
$ {bin} start
----

[#troubleshooting_bundle_version_mismatch]
=== Troubleshooting bundle version mismatch

Created {prod} instances contain bundle information and instance data.
Bundle information and instance data is not updated when setting up a new {prod} release.
This information is not updated due to customization in the earlier instance data.
This will lead to errors when running the [command]`{bin} start` command:

[subs="+quotes,attributes"]
----
$ {bin} start
...
FATA Bundle 'crc_hyperkit_4.2.8.crcbundle' was requested, but the existing VM is using
'crc_hyperkit_4.2.2.crcbundle'
----

.Procedure
. Issue the [command]`{bin} delete` command before attempting to start the instance:
+
[subs="+quotes,attributes"]
----
$ {bin} delete
----
+
[WARNING]
====
The [command]`{bin} delete` command results in the loss of data stored in the {prod} instance.
Save any desired information stored in the instance before running this command.
====

[#troubleshooting_unknown_issues]
=== Troubleshooting unknown issues

Resolve most issues by restarting {prod} with a clean state.
This involves stopping the instance, deleting it, reverting changes made by the [command]`{bin} setup` command, reapplying those changes, and restarting the instance.

.Prerequisites
* You set up the host machine with the [command]`{bin} setup` command.
For more information, see link:{crc-gsg-url}#setting-up_gsg[Setting up {prod}].
* You started {prod} with the [command]`{bin} start` command.
For more information, see link:{crc-gsg-url}#starting-the-instance_gsg[Starting the instance].
* You are using the latest {prod} release.
Using a version earlier than {prod} 1.2.0 might result in errors related to expired x509 certificates.
For more information, see link:{crc-gsg-url}#troubleshooting-expired-certificates_gsg[Troubleshooting expired certificates].

.Procedure
To troubleshoot {prod}, perform the following steps:

. Stop the {prod} instance:
+
[subs="+quotes,attributes"]
----
$ {bin} stop
----

. Delete the {prod} instance:
+
[subs="+quotes,attributes"]
----
$ {bin} delete
----
+
[WARNING]
====
The [command]`{bin} delete` command results in the loss of data stored in the {prod} instance.
Save any desired information stored in the instance before running this command.
====

. Clean up remaining changes from the [command]`{bin} setup` command:
+
[subs="+quotes,attributes"]
----
$ {bin} cleanup
----
+
[NOTE]
====
The [command]`{bin} cleanup` command removes an existing {prod} instance and reverts changes to DNS entries created by the [command]`{bin} setup` command.
On {mac}, the [command]`{bin} cleanup` command also removes the system tray.
====

. Set up your host machine to reapply the changes:
+
[subs="+quotes,attributes"]
----
$ {bin} setup
----

. Start the {prod} instance:
+
[subs="+quotes,attributes"]
----
$ {bin} start
----
+
[NOTE]
====
The cluster takes a minimum of four minutes to start the necessary containers and Operators before serving a request.
====

If your issue is not resolved by this procedure, perform the following steps:

. link:https://github.com/crc-org/crc/issues[Search open issues] for the issue that you are encountering.
. If no existing issue addresses the encountered issue, link:https://github.com/crc-org/crc/issues/new[create an issue] and link:https://help.github.com/en/articles/file-attachments-on-issues-and-pull-requests[attach the [filename]*_~/.crc/crc.log_* file] to the created issue.
The [filename]*_~/.crc/crc.log_* file has detailed debugging and troubleshooting information which can help diagnose the problem that you are experiencing.

